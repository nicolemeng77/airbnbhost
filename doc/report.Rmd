---
title: "Airbnb Pricing Plan"
author: "Group 13 - Shuang Lu, Hongye Jiang, Chengyue Meng, Siyu Yang, Shaofu Wang"
date: "11/22/2019"
output: pdf_document
---

```{r setup, include=FALSE}
load("../data/orig_data.RData")
library(tidyverse)
library(MASS)
library(scales)
library(data.table)
library(rsample) 
library(randomForest)
library(glmnet)
library(ranger) 
library(gbm)
library(xgboost)
library(caret)
library(lime)
library(vtreat)
library(modelr)
library(broom)
library(MLmetrics)
library(tidyr)
library(tidymodels)
```

# Part 1: Introduction       

New York City is one of the most attactive metropolitan city for tourism, thus has fostered the Airbnb market and provide huge financial benefits for individual hosts. While making it possible to earn money without establishing a real business, there are lots of problems and subtleties when setting a reasonable price. Are hosts really making money considering all aspects apart from the most obvious one - location? Is the property really worth the price or is it just because the host is good at advertising? Do you think providing shampoo will affect the price? Thus, our project will investigate the relationship between price and a range of pricing factors. We will be solving the problem from the perspective of the hosts and in particular, recommend a reasonable price or price range for recently joined ones.      


In our raw data, there are `r nrow(orig_data)` rows, each presenting a listing and `r ncol(orig_data)` columns representing possible pricing factors. Briefly browsing the columns and we have done an initial selection of factors, removing columns such as host's name, host's location and same values shared among all hosts such as state and country information which is NY, USA for all.            


```{r, include=FALSE}
#ignore useless variables
data <- orig_data %>% dplyr::select(id,name,description, transit, house_rules, host_since, host_is_superhost, 
                             host_listings_count, host_identity_verified, neighbourhood_cleansed, square_feet,
                             neighbourhood_group_cleansed, zipcode, latitude, longitude, property_type, 
                             room_type, accommodates, bathrooms, bedrooms, beds,bed_type, amenities, price,
                             weekly_price, cleaning_fee, minimum_nights, availability_365,
                             number_of_reviews, instant_bookable, cancellation_policy, 
                             require_guest_phone_verification,
                             host_response_time, host_verifications, security_deposit)
```


```{r}
nrow(data)
ncol(data)
#colnames(data)
```

Now we are left with `r ncol(data)` columns, including numerical variables (eg: number of bedrooms/bathrooms); categorial variables(eg: neighbourhood and property types) and text variables (eg: name and description).          


# Part 2: Data Pre-processing

```{r}
#rename variable
data <- data %>% 
  rename(neighbourhood = neighbourhood_cleansed, 
         neighbourhood_group = neighbourhood_group_cleansed) 
```


```{r}
#to lower letter, delete sign,
data <- data %>%
  mutate(amenities = tolower(amenities)) %>% 
  mutate(house_rules = tolower(house_rules)) %>% 
  mutate(name = tolower(name)) 

convert_price <- function(x) as.numeric(gsub("\\$|,", "", x))
string_split <- function(x) stringr::str_split(gsub("\\{|\\}|\"", "", x),",")

data <- data %>%
  mutate_at(c("price", "weekly_price", "cleaning_fee", "security_deposit"), convert_price) %>%
  mutate_at("amenities", string_split) %>%
  mutate_at('host_verifications', string_split) 

#add new var
data <- data %>% 
  mutate(hosted_years = round(as.double(difftime(Sys.Date(), host_since)) / 365, 1)) %>%
  mutate(number_of_amenities = lengths(amenities)) %>%
  mutate(number_of_verifications = lengths(host_verifications)) %>%
  mutate(transit_level = as.numeric(is.na(transit))) %>%  #1:has description, 0:NA
    #amenity
  mutate(rule_pets = ifelse(grepl("no pets", data$house_rules), 0, 1)) %>%
  mutate(have_pet = ifelse(grepl("cat", data$amenities) | 
                                grepl("dog", data$amenities) | 
                                grepl("pet", data$amenities), 1, 0)) %>%
  mutate(have_shampoo = ifelse(grepl("shampoo", data$amenities), 1, 0)) %>%
  mutate(have_gym = ifelse(grepl("gym", data$amenities), 1, 0)) %>%
  mutate(have_washer = ifelse(grepl("washer", data$amenities), 1, 0)) %>%
  mutate(have_kitchen = ifelse(grepl("kitchen", data$amenities), 1, 0)) %>%
  mutate(have_parking = ifelse(grepl("parking", data$amenities), 1, 0)) %>% 
  mutate(have_elevator = ifelse(grepl("elevator", data$amenities), 1, 0)) %>% 
  mutate(have_laptop_workspac = ifelse(grepl("laptop friendly workspace", data$amenities), 1, 0))



#fill NA (to 0 in most cases)
data <- data %>%
  mutate(host_is_superhost = ifelse(is.na(host_is_superhost), 0, host_is_superhost)) %>%
  mutate(bathrooms = ifelse(is.na(bathrooms), 0, bathrooms)) %>%
  mutate(bedrooms = ifelse(is.na(bedrooms), 0, bedrooms)) %>%
  mutate(beds = ifelse(is.na(beds), 0, beds)) %>%
  mutate(number_of_reviews = ifelse(is.na(number_of_reviews), 0, number_of_reviews)) %>%
  mutate(host_response_time = ifelse(is.na(host_response_time), 'no_record', host_response_time)) %>%
  mutate(cleaning_fee = ifelse(is.na(cleaning_fee), 0, cleaning_fee)) %>%
  mutate(security_deposit = ifelse(is.na(security_deposit), 0, security_deposit)) %>%
  mutate(host_listings_count = ifelse(is.na(host_listings_count), 0, host_listings_count)) %>%
  mutate(host_identity_verified = ifelse(is.na(host_identity_verified), 0, host_identity_verified)) %>%
  mutate(hosted_years = ifelse(is.na(hosted_years), 0, hosted_years)) 


#change logical to numeric, as.factor
data <- data %>% 
  mutate_if(is.logical, as.numeric) %>% # convert True False to 1,0
  mutate_at(c("neighbourhood", "neighbourhood_group", "property_type", 
              "room_type", "bed_type", "cancellation_policy", 'host_is_superhost', 
              'host_identity_verified', 'instant_bookable', 'require_guest_phone_verification',
              'host_response_time', 'transit_level', 'rule_pets', 'have_pet', 'have_shampoo', 
              'have_gym', 'have_washer', 'have_kitchen', 'have_parking', 'have_elevator', 
              'have_laptop_workspac'), 
            as.factor)

#combine property_type ( factors)
data <- data.table(data)
data <- data[property_type %in% c("Barn", "Bed and breakfast", "Boat", "Bungalow","Bus","Cabin","Camper/RV","Casa particular (Cuba)", "Castle","Cave","Cottage","Dome house","Earth house","Farm stay","Houseboat","Island","Nature lodge","Other","Tent","Timeshare","Tiny house","Yurt"), property_type:= "Others"]
data <- data[property_type %in% c("Townhouse", "House", "Guesthouse", "Resort","Villa","Tiny house"), property_type:= "House"]
data <- data[property_type %in% c("Apartment","Condominium","Guest suite","Serviced apartment"), property_type:= "Apartment"]
data <- data[property_type %in% c("Hostel","Hotel","Boutique hotel","Aparthotel"), property_type:= "Hotel"]
data <- data[grepl("loft", data$name), property_type := "Loft"]
```



In terms of data pre-processing, we have done some simple formatting such as removing dollar sign from price columns and change characters to numeric mode. Besides, the original data sets include over 30 property types, so we have merged similar ones such as "Apartment" and "Serviced apartment" and treat it as factor with only 5 levels: Apartments, Hotel, Loft, House and Others.       


In order to test whether a specific amenity such as shampoo, parking or kitchen will have significant effect on pricing. We have created new dummy variables which have the names "have_shampoo" or "have_parking". Additionally, for later modelling, we have done one-hot encoding for categorical variables with more than two levels. For instance, for neighbourhood group,       




(Also, for NLP, we have removed stopwords, punctuations and special characters from the "description" and "summary". We also have extracted their stems for further text analysis.  )              


Looking at the histogram of price, we noticed that there are several properties with abnormally high price which will potentially impact modelling fitting and sample tests. It is natural to remove them from the dataset. Furthermore, we have removed 1 listing that has over 20 bedrooms and 5 listings with more than 20 beds. 
```{r}
#Distribution of Price
ggplot(aes(price), data = data) +labs(  title = "Histogram of price", xlab = "Price") + 
  geom_histogram(alpha=0.8, aes(y=..density..), fill="lightblue", col="white") +
   geom_density( col = "red") + 
  theme_minimal()

#jpeg("rplot.jpg")

ggplot(aes(log(price)), data = data) +labs(  title = "Histogram of log price", xlab = "Price") + 
  geom_histogram(alpha=0.8, aes(y=..density..), fill="lightblue", col="white") +
   geom_density( col = "red") + 
  theme_minimal()

#dev.off()
```


```{r}
# average price by room type + boroughs
price_type <- data %>% group_by(neighbourhood_group, room_type) %>% summarise(n=n())
setorderv(price_type, cols = "n", order = -1)
ggplot(price_type, aes(fill=room_type, y=n, x=neighbourhood_group)) +
        geom_bar(position="stack", stat="identity")+
        xlab("Neighbourhood") + ylab("# of Listings")
```



```{r}
# Unusual cases

nrow(data)  #48377
# show deleted numbers for each filer
# nrow(data %>% filter(price > 2000)) #81
# nrow(data %>% filter(price == 0 )) #10
# nrow(data %>% filter(cleaning_fee >= 200 & price <= 100 & minimum_nights <= 10)) #37
# nrow(data %>% filter(price >= 2000 & bathrooms <= 1 & accommodates <= 8)) #46
# nrow(data %>% filter(minimum_nights > 90)) #361
# nrow(data %>% filter(property_type == "Others")) #309
```

```{r}
#deal with unusual cases
data1 <- data  %>% 
  filter(price > 0,
         cleaning_fee < 200 | price > 100 | minimum_nights > 10, 
         price < 2000 | bathrooms > 1 | accommodates > 8,
         minimum_nights <= 90)
nrow(data1)  #47929

#last version
# data <- data  %>% 
#   filter(beds <= 20, bedrooms <= 20,
#          bedrooms != 1 | bathrooms <= 4, 
#          price < 1500 & price > 0,
#          cleaning_fee <= 200 | minimum_nights >= 30,
#          minimum_nights <= 90) %>%
#   filter(property_type != "Others") #havn't decided
```

```{r}
#Outliers

```



```{r}
#check missing value
colSums(is.na(data))  #21 NAs in host_since, host_listings_count, host_identity_verified, hosted_years
```
delete weekly_price, square_feet

Next, we have to deal with missing values. It is natural to think that a property's area is highly associated with price and hence an efficient predictor. Unfortunately, in our dataset, there are over 90% of the listings that do not have that information. Therefore, we have no choice but delete the "square_feet" variable. But variables like number of bedrooms/bathrooms and room type are still variable, and are able to somewhat represent information of areas. 


# Part 3: Exploratory Date Analysis

```{r}
###neighbourhood_group###
# % of 5 boroughs
dat2 <- data.table(data)
neighb_count <- dat2[, .N, keyby = neighbourhood_group] %>%
                mutate(percent = percent(N / sum(N))) %>% 
                mutate(label = paste(neighbourhood_group, percent))
pie(neighb_count$N, neighb_count$label)
# average price in 5 boroughs
price_avg <- dat2 %>% group_by(neighbourhood_group) %>% 
                 summarise(Avg_price = round(mean(price, na.rm = T),2), N = n())
setorderv(price_avg, "Avg_price",order = -1)
price_avg
#bar plot
barplot(price_avg$Avg_price, names.arg = price_avg$neighbourhood_group, ylim = c(0,200))
text(x = c(0.7,1.9,3.1,4.2,5.5), y = price_avg$Avg_price + 6, labels = paste("$", price_avg$Avg_price))
#hist of price in 5 boroughs
data %>% ggplot(aes(price, color = neighbourhood_group)) + geom_freqpoly()
#boxplot
data %>% ggplot(aes(fct_reorder(neighbourhood_group, -price), price, color = neighbourhood_group)) + geom_boxplot()
#Wilcoxon rank-sum test
kruskal.test(price ~ neighbourhood_group, data)

pairwise.wilcox.test(data$price, data$neighbourhood_group)
```


```{r}
###neighbourhood###
dat_m <- dat2[neighbourhood_group %in% c("Manhattan", "Brooklyn")]
m_count <- dat_m[,.N,keyby = neighbourhood]
setorderv(m_count,"N",order = -1)
m_count
barplot(m_count$N[1:5], names.arg = m_count$neighbourhood[1:5],cex.names = 0.9)
```


```{r}
###room_type###
# average price by room type  
price_type <- dat2 %>% group_by(room_type) %>% summarise(avg_price = round(mean(price, na.rm = T),2))
price_type
barplot(height = price_type$avg_price, names.arg = price_type$room_type)
# average price by room type + boroughs
price_type <- data %>% group_by(neighbourhood_group, room_type) %>% summarise(n=n())
setorderv(price_type, cols = "n", order = -1)
ggplot(price_type, aes(fill=room_type, y=n, x=neighbourhood_group)) +
        geom_bar(position="stack", stat="identity")+
        xlab("Neighbourhood") + ylab("Counts")
#Wilcoxon rank-sum test
kruskal.test(price ~ room_type, data)
#p-value < 2.2e-16, significant differences in price according to room types
```


```{r}
###hosted_years###
#avg hosted years for five boros
#any relationship between hosted years and avg price
data %>% group_by(neighbourhood_group) %>% summarise(avg_hosted_years = mean(hosted_years,na.rm = T),avg_price = mean(price)) %>% arrange(desc(avg_price))
```


Else
```{r}
###cancellation_policy###
#see whether the cancellation policy is strict or moderate
data = data %>%
  mutate(cancelpolicy = strsplit(as.character(data$cancellation_policy),"_"))
how_is_cancellation = function(x){
  ifelse("strict" %in% x,"strict",ifelse("moderate" %in% x,"moderate","flexible"))
}
#for example, see how is the cancellation polict of the first 7 host
#unlist(lapply(data$cancelpolicy,how_is_cancellation)[1:7])
name = data$name[1:7]
cancel = unlist(lapply(data$cancelpolicy,how_is_cancellation)[1:7])
as.data.frame(cbind(name,cancel))
```


```{r}
#neighbourhood
#assume same shape, only differ in location
data %>% ggplot(aes(price, color = neighbourhood_group)) + geom_freqpoly()
kruskal.test(price ~ neighbourhood_group, data)
#with no surprise, the price in at least one borough is different from others
wilcox.test(data[data$neighbourhood_group == 'Staten Island',]$price, data[data$neighbourhood_group == 'Queens',]$price)
#p-value = 0.3391, no significant difference of prices in Staten Island and queens
#two sample test
t.test(data[data$neighbourhood_group == 'Staten Island',]$price,data[data$neighbourhood_group == 'Queens',]$price)
#p-value = 0.9255, no significant difference of prices in Staten Island and queens
#room_type in Manhattan
#assume shape of dist same
data[data$neighbourhood_group == "Manhattan",] %>% ggplot(aes(price, color = room_type)) + geom_freqpoly()
kruskal.test(price~room_type, data = data[data$neighbourhood_group == "Manhattan",])
#p-value < 2.2e-16, significant difference of prices among room type in Manhattan
summary(aov(price~room_type,data[data$neighbourhood_group == "Manhattan",]))
#p-value small, significant difference of prices among room type in Manhattan
Box.test(aov(price~room_type,data[data$neighbourhood_group == "Manhattan",])$residuals)
#error variance is not constant
dwtest(aov(price~room_type,data[data$neighbourhood_group == "Manhattan",]))
#errors are correlated,remedial measures: Transformation, Cochrane-Orcutt Procedure
#property_type in Manhattan
man = data[data$neighbourhood_group == "Manhattan",]
wilcox.test(man[man$property_type == 'Apartment',]$price, 
           man[man$property_type == 'Others',]$price)
#for example, there is no significant differences in prices between apartment and others in Manhattan.
#linear regression
avg_price_hostedyears = data %>% group_by(neighbourhood_group) %>% summarise(avg_hosted_years = mean(hosted_years,na.rm = T),avg_price = mean(price)) %>% arrange(desc(avg_price))
f = lm(avg_price_hostedyears$avg_price~avg_price_hostedyears$avg_hosted_years)
summary(f)
#R square small not so significant relationship btw avg price and avg hosted years
qqnorm(f$residuals)
qqline(f$residuals)
#the residuals are not normally distributed
#predict price
#select all numerial variables
fit <- lm(price ~ host_is_superhost +host_identity_verified + accommodates+ bathrooms + bedrooms + beds +cleaning_fee+minimum_nights+availability_365+ number_of_reviews + instant_bookable + require_guest_phone_verification + hosted_years + number_of_amenities, data)
summary(fit)
sort(car::vif(fit),decreasing = TRUE)
qqnorm(fit$residuals)
#no multicollinearity > 10 variables
#There is skewness, many points off the normal line. Thus the residuals are not normally distributed.
#remedial measures: transformation, robust regression methods
library(lmtest)
dwtest(fit)
#p-value smaller than 0.05, so we reject the null hypothests. The errors are correlated. remedial measures: Transformation, Cochrane-Orcutt Procedure
#model selection
step(fit,direction = "both")
#accommodates,bathrooms, bedrooms,beds,cleaning_fee,min_nights,number of reviews, number of amenities, hosted years and number of amenities
summary(lm(price ~accommodates+ bathrooms + bedrooms + beds +cleaning_fee+minimum_nights+ number_of_reviews + number_of_amenities+hosted_years, man))
# R square 0.4164 bigger than that of the model using all numerical variables.
# Here using less vairables, model better
```


# Part 4: Predictive Modeling     

## Linear Regression

Since the model output, price, is a continuous variable, the first baseline model we have chosen is linear regression 

```{r}
random_list <- sample(nrow(data),round(nrow(data)*0.7))
train <- data[random_list,]
test <- data[-random_list,]
```


```{r}
#linear regression
data <- data %>% 
  mutate(if_Manhattan = ifelse(data$neighbourhood_group == 'Manhattan', 1, 0)) %>%
  mutate(if_Brooklyn = ifelse(data$neighbourhood_group == 'Brooklyn', 1, 0)) %>%
  mutate(if_Queens = ifelse(data$neighbourhood_group == 'Queens', 1, 0)) %>%
  mutate(if_Bronx = ifelse(data$neighbourhood_group == 'Bronx', 1, 0)) 

data_linear <- data %>% 
  mutate_at(c('host_is_superhost', 'host_identity_verified', 'instant_bookable', 'require_guest_phone_verification',
              'transit_level', 'rule_pets', 'have_pet', 'have_shampoo', 
              'have_gym', 'have_washer', 'have_kitchen', 'have_parking', 'have_elevator', 
              'have_laptop_workspac'), function(x) as.numeric(x)-1)

train_linear <- data_linear[random_list,]
test_linear <- data_linear[-random_list,]

fit_linear <- lm(log(price) ~ host_is_superhost + host_listings_count 
           + host_identity_verified + latitude + longitude
           + accommodates + bathrooms + bedrooms + beds
           + minimum_nights + availability_365
           + security_deposit + hosted_years + number_of_amenities
           + number_of_verifications + transit_level + rule_pets
           + have_pet + have_shampoo + have_gym
           + have_washer + have_kitchen + have_parking
           + have_elevator + have_laptop_workspac 
           + if_Manhattan + if_Brooklyn + if_Queens + if_Bronx
           + number_of_reviews, train_linear)

summary(fit_linear)
#Adjusted R-squared:  0.5317

MSE(predict(fit_linear, test_linear), test_linear$price)
#MSE: 48436.03

#multicollinearity
car::vif(fit_linear)

step_result <- stepAIC(fit_linear)

fit_linear1 <- lm(formula = log(price) ~ host_is_superhost + host_listings_count + 
    latitude + longitude + accommodates + bedrooms + beds + minimum_nights + 
    availability_365 + security_deposit + hosted_years + number_of_amenities + 
    number_of_verifications + transit_level + rule_pets + have_pet + 
    have_shampoo + have_gym + have_washer + have_kitchen + have_parking + 
    have_elevator + if_Manhattan + if_Brooklyn + if_Queens + 
    if_Bronx + number_of_reviews, data = train_linear)

#delete host_identity_verified, bathrooms, have_laptop_workspac

```

## Regularized Linear Regression

When number of features is large and suffer from collinearity issues, simple linear model tends to overfit and thus have high variance sample. Hence, we next conducted lasso regression as a means of regularization in order to perform feature selection, reduce variance and decrease our of sample error.

```{r}
### LASSO ###
lasso_cv <- cv.glmnet(
  x = data.matrix(train[,c('host_is_superhost', 'host_listings_count', 'host_identity_verified', 'latitude', 'longitude', 'accommodates', 'bathrooms', 'bedrooms', 'beds', 'minimum_nights', 'availability_365', 'security_deposit', 'hosted_years', 'number_of_amenities', 'number_of_verifications', 'transit_level', 'rule_pets', 'have_pet', 'have_shampoo', 'have_gym', 'have_washer', 'have_kitchen', 'have_parking', 'have_elevator', 'have_laptop_workspac', 'if_Manhattan', 'if_Brooklyn', 'if_Queens', 'if_Bronx', 'number_of_reviews')]), 
  y = train$price,
  alpha = 1
)

lasso_reg <- glmnet(
  x = data.matrix(train[,c('host_is_superhost', 'host_listings_count', 'host_identity_verified', 'latitude', 'longitude', 'accommodates', 'bathrooms', 'bedrooms', 'beds', 'minimum_nights', 'availability_365', 'security_deposit', 'hosted_years', 'number_of_amenities', 'number_of_verifications', 'transit_level', 'rule_pets', 'have_pet', 'have_shampoo', 'have_gym', 'have_washer', 'have_kitchen', 'have_parking', 'have_elevator', 'have_laptop_workspac', 'if_Manhattan', 'if_Brooklyn', 'if_Queens', 'if_Bronx', 'number_of_reviews')]), 
  y = train$price,
  alpha = 1,
  lambda=lasso_cv$lambda.min
)

pred_lasso <- predict(lasso_reg, data.matrix(test[,c('host_is_superhost', 'host_listings_count', 'host_identity_verified', 'latitude', 'longitude', 'accommodates', 'bathrooms', 'bedrooms', 'beds', 'minimum_nights', 'availability_365', 'security_deposit', 'hosted_years', 'number_of_amenities', 'number_of_verifications', 'transit_level', 'rule_pets', 'have_pet', 'have_shampoo', 'have_gym', 'have_washer', 'have_kitchen', 'have_parking', 'have_elevator', 'have_laptop_workspac', 'if_Manhattan', 'if_Brooklyn', 'if_Queens', 'if_Bronx', 'number_of_reviews')]), )

MSE(pred_lasso, test$price)
#MSE: 18118.49

y = test$price
sst <- sum((y - mean(y))^2)
sse <- sum((pred_lasso - y)^2)

# R squared
rsq <- 1 - sse / sst
rsq #0.3549032
```


## Random Forest

Next we train a random forest regression model since it works well with categorical variables and missing values.
We prepared model data by splitig dataset as 70% train and 30% test and we used OOB (out-of-bag) error as validation error rate.

```{r}
model_data <- data %>%  dplyr::select(host_is_superhost,host_listings_count,
                              host_identity_verified,
                      neighbourhood_group, property_type, room_type, accommodates, 
                      bathrooms, bedrooms, beds, bed_type, price, minimum_nights,
                      availability_365, instant_bookable, cancellation_policy, 
                      require_guest_phone_verification, host_response_time, 
                      number_of_amenities, number_of_verifications, transit_level,
                      rule_Pets, have_pet, have_shampoo, have_gym, have_washer,
                      have_kitchen, have_parking, have_elevator, have_laptop_workspac)

set.seed(0)
split <- rsample::initial_split(model_data, prop = 0.7)
data_train <- training(split)
data_test <- testing(split)
```

Then we fitted a basic random forest model with *ranger* package. We pick number of trees to be 500 as default and  *mtry* parameters, which is number of features to pick for each split, to be $\frac{\# of features}{3}$ as default optimal number.

```{r}
rf <- ranger(
  formula   = price ~ ., 
  data      = data_train, 
  num.trees = 500,
  mtry      = floor((ncol(model_data)-1) / 3))
```


For this random forest model, we achieved `r rf$r.squared` OOB R squared and `r rf$prediction.error` OOB MSE. It already outperformed linear regression, but we still needed to fine tune model parameters to further improve predicting accuracy. Hence we conducted hyperparameter tuning through full grid search.

```{r}
hyper_grid <- expand.grid(
  mtry       = c(6,9,12),
  node_size  = c(3,5,7),
  sampe_size = c(.632, .80),
  OOB_RMSE   = 0
)

for(i in 1:nrow(hyper_grid)) {

  model <- ranger(
    formula         = price ~ .,
    data            = data_train,
    num.trees       = 500,
    mtry            = hyper_grid$mtry[i],
    min.node.size   = hyper_grid$node_size[i],
    sample.fraction = hyper_grid$sampe_size[i],
    seed            = 0
  )

  # add OOB error to grid
  hyper_grid$OOB_RMSE[i] <- sqrt(model$prediction.error)
}
 
hyper_grid %>%
  dplyr::arrange(OOB_RMSE) %>%
  head(10)
```

The grid we constructed contains `r nrow(hyper_grid)` number of parameters combinations.
Our tuning result shows that the optimal parameters to be num.trees = 500, mtry = 9 , 
min.node.size = 3 and sample.fraction = 0.8 for our model.

```{r}
optimal_ranger <- ranger(
  formula         = price ~ ., 
  data            = data_train, 
  num.trees       = 500,
  mtry            = 9,
  min.node.size   = 3,
  sample.fraction = .8,
  importance      = 'impurity'
)
```

```{r}
optimal_ranger$variable.importance %>% 
  tidy() %>% 
  dplyr::arrange(desc(x)) %>%
  dplyr::top_n(15) %>%
  ggplot(aes(reorder(names, x), x)) +
  geom_col(fill = "lightblue") +
  ggtitle("Top 15 important variables")+
  xlab("")+
  ylab("")+
  coord_flip() +
  theme_classic()
```

From the variable importance plot, we can see that top 3 variables picked up by random forest model is 
room_type, accomodates, bathrooms.


```{r}
pred <- predict(optimal_ranger, data_test)
MSE_RF <- mean((pred$predictions - data_test$price)^2)
MSE_RF
```

We achieved final test MSE of `r MSE_RF` by random forest model.



## XGBoost

Then we move on to xgboost model in hope to further improve prediction power. Whereas random forests build an ensemble of deep independent trees, XGBoosts build an ensemble of shallow and weak successive trees with each tree learning and improving on the previous. When combined, these many weak successive trees produce a powerful “committee” that are often hard to beat with other algorithms.

For data preparation, we performed one-hot encoding for categorical variables using *vtreat* package since
XGBoost only works with matrices that contain all numeric variables.

```{r}
# variable names
features <- setdiff(names(data_train), "price")

# Create the treatment plan from the training data
treatplan <- vtreat::designTreatmentsZ(data_train, features, verbose = FALSE)

# Get the "clean" variable names from the scoreFrame
new_vars <- treatplan %>%
  magrittr::use_series(scoreFrame) %>%        
  dplyr::filter(code %in% c("clean", "lev")) %>% 
  magrittr::use_series(varName)     

# Prepare the training data
features_train <- vtreat::prepare(treatplan, data_train, varRestriction = new_vars) %>% as.matrix()
response_train <- data_train$price

# Prepare the test data
features_test <- vtreat::prepare(treatplan, data_test, varRestriction = new_vars) %>% as.matrix()
response_test <- data_test$price
```


```{r}
set.seed(0)

xgb.fit <- xgb.cv(
  data = features_train,
  label = response_train,
  nrounds = 1000,
  nfold = 5,
  objective = "reg:linear",  # for regression models
  verbose = 0,               # silent,
  early_stopping_rounds = 10 # stop if no improvement for 10 consecutive trees
)

xgb.fit$evaluation_log %>%
  dplyr::summarise(
    ntrees.train = which(train_rmse_mean == min(train_rmse_mean))[1],
    rmse.train   = min(train_rmse_mean),
    ntrees.test  = which(test_rmse_mean == min(test_rmse_mean))[1],
    rmse.test   = min(test_rmse_mean)
  )
```

```{r}
# plot error vs number trees
ggplot(xgb.fit$evaluation_log) +
  geom_line(aes(iter, train_rmse_mean), color = "red") +
  geom_line(aes(iter, test_rmse_mean), color = "blue")
```

From the plot above, we can see how training and test RMSE decreases as the number of trees grows. 
And Test RMSE 74.93852 reached minumun as 74.93852. 

```{r, eval=False}
## TODO ## 
## Need to update this grid ##
hyper_grid <- expand.grid(
  eta = c(0.01, 0.05, 0.1),
  max_depth = c(3,5, 7), 
  min_child_weight = c(1, 3, 5), 
  subsample = c(0.65, 0.8), 
  colsample_bytree = c(.8, 0.9, 1), 
  optimal_trees = 0,               # a place to dump results
  min_RMSE = 0                     # a place to dump results
)

for(i in 1:nrow(hyper_grid)) {
  
  # create parameter list
  params <- list(
    eta = hyper_grid$eta[i],
    max_depth = hyper_grid$max_depth[i],
    min_child_weight = hyper_grid$min_child_weight[i],
    subsample = hyper_grid$subsample[i],
    colsample_bytree = hyper_grid$colsample_bytree[i]
  )
  
  set.seed(0)
  
  # train model
  xgb.tune <- xgb.cv(
    params = params,
    data = features_train,
    label = response_train,
    nrounds = 5000,
    nfold = 5,
    objective = "reg:linear",  # for regression models
    verbose = 0,               # silent,
    early_stopping_rounds = 10 # stop if no improvement for 10 consecutive trees
  )
  
  # add min training error and trees to grid
  hyper_grid2$optimal_trees[i] <- which.min(xgb.tune$evaluation_log$test_rmse_mean)
  hyper_grid2$min_RMSE[i] <- min(xgb.tune$evaluation_log$test_rmse_mean)
} 

hyper_grid %>% 
  dplyr::arrange(min_RMSE) %>%
  head(10)
```

We again tuned our xgoost model parameters through grid search and find the optimal paramters to be :
eta(learning rate) = 0.01, max_depth = 7, min_child_weight, sub_sampling = , colsample_bytree = .

```{r}
## TODO## 
tuned_params <- list(
  eta = 0.01,
  max_depth = 7,
  min_child_weight = 1,
  subsample = 0.65,
  colsample_bytree = 0.9
)

xgb.final <- xgboost(
  params = tuned_params,
  data = features_train,
  label = response_train,
  nrounds = 5000,
  objective = "reg:linear",  # for regression models
  verbose = 0,               # silent,
  early_stopping_rounds = 10 # stop if no improvement for 10 consecutive trees
)

pred <- predict(xgb.final, features_test)

# results
MSE_XGBoost <- (caret::RMSE(pred, response_test))^2
MSE_XGBoost
```

We then fed the optimal model parameters to test dataset and achieved test MSE to be 68.57523, which is slightly improved from the random forest model.

```{r}
# create importance matrix
importance_matrix <- xgb.importance(model = xgb.final)

# variable importance plot
xgb.plot.importance(importance_matrix, top_n = 10, measure = "Gain")
```

For XGBoost model, the top 3 influential variables are accommodates, bathrooms, availability_365 (Similar or different from random forest)

```{r}
### TODO ###
## Optional Include LIME visualizaiton 

# # one-hot encode the local observations to be assessed.
# local_obs_onehot <- vtreat::prepare(treatplan, local_obs, varRestriction = new_vars)
# 
# # apply LIME
# explainer <- lime(data.frame(features_train), xgb.fit.final)
# explanation <- explain(local_obs_onehot, explainer, n_features = 5)
# plot_features(explanation)
```



