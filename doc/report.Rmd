---
title: "report"
author: "Chengyue Meng, Shuang Lu"
date: "11/22/2019"
output: pdf_document
---

```{r setup, include=FALSE}
load("../data/orig_data.RData")
library(tidyverse)
library(MASS)
library(scales)
library(data.table)
library(rsample) 
library(randomForest)
library(ranger) 
library(gbm)
library(xgboost)
library(caret)
library(lime)
library(vtreat)
library(modelr)
library(broom)
library(tidyr)
library(tidymodels)
```

# Part 1: Introduction       

New York City is one of the most attactive metropolitan city for tourism, thus has fostered the Airbnb market and provide huge financial benefits for individual hosts. While making it possible to earn money without actually establishing a "business", there are lots of problems and subtleties when setting the correct price. Are hosts really making money considering all aspects apart from the most obvios factor - location? Is the property really worth the price or is it just because the host is good at advertising? Do you think providing shampoo will affect the price? Thus, our project will investigate the relationship between price and a range of pricing factors. We will be solving the problem from host's perspective instead of customer's and recommend a reasonable price or price range for the listed properties.      


There are `r nrow(orig_data)` rows, each row presenting a listing and `r ncol(orig_data)` columns representing a possible factor. Briefly browsing the columns and we have done an initial selection of factors, removing columns such as host's name, host's location and same values shared among all hosts such as state and country information which is NY, USA for all.            


```{r}
#ignore useless variables
data <- orig_data %>% dplyr::select(id,name,description, transit, house_rules, host_since, host_is_superhost, 
                             host_listings_count, host_identity_verified, neighbourhood_cleansed, square_feet,
                             neighbourhood_group_cleansed, zipcode, latitude, longitude, property_type, 
                             room_type, accommodates, bathrooms, bedrooms, beds,bed_type, amenities, price, weekly_price, cleaning_fee, minimum_nights, availability_365,
                             number_of_reviews, instant_bookable, cancellation_policy, 
                             require_guest_phone_verification,
                             host_response_time, host_verifications, security_deposit)
#host_has_profile_pic,  host_identity_verified
#or just read the condensed listings(with different filters)
#data = read.csv("listings_condensed.csv",header = T)                            
```




```{r}
nrow(data)
colnames(data)
```

Now we are left with `r ncol(data)` columns: `r colnames(data)`. The data contains numerical variables: number of bedrooms/bathrooms; categorial variables: neighbourhood, property types and text variables: name/description.          


# Part 2: Data Pre-processing

```{r}
#rename variable
data <- data %>% 
  rename(neighbourhood = neighbourhood_cleansed, 
         neighbourhood_group = neighbourhood_group_cleansed) 
```


```{r}
#data structure
data <- data %>% 
  mutate_if(is.logical, as.numeric) %>% # convert True False to 1,0
  mutate_at(c("neighbourhood", "neighbourhood_group", "property_type", 
              "room_type", "bed_type", "cancellation_policy", 'host_is_superhost', 
              'host_identity_verified', 'instant_bookable', 'require_guest_phone_verification',
              'host_response_time'), 
            as.factor)
```




```{r}
#edit data value
convert_price <- function(x) as.numeric(gsub("\\$|,", "", x))
string_split <- function(x) str_split(gsub("\\{|\\}|\"", "", x),",")
data <- data %>%
  mutate_at(c("price", "weekly_price", "cleaning_fee", "security_deposit"), convert_price) %>%
  mutate_at("amenities", string_split) %>%
  mutate_at('host_verifications', string_split) %>% 
  mutate(hosted_years = round(as.double(difftime(Sys.Date(), host_since)) / 365, 1))
data <- data %>% 
  mutate(number_of_amenities = lengths(amenities)) %>%
  mutate(number_of_verifications = lengths(host_verifications))
#amenity
data <- data %>% 
  mutate(amenities = tolower(amenities)) %>% 
  mutate(house_rules = tolower(house_rules)) %>% 
  mutate(name = tolower(name)) %>%
  mutate(transit_level = as.numeric(is.na(transit))) %>%  #1:has description, 0:NA
  mutate(rule_Pets = ifelse(grepl("no pets", data$house_rules), 0, 1)) %>%
  mutate(have_pet = ifelse(grepl("cat", data$amenities) | 
                                grepl("dog", data$amenities) | 
                                grepl("pet", data$amenities), 1, 0)) %>%
  mutate(have_shampoo = ifelse(grepl("shampoo", data$amenities), 1, 0)) %>%
  mutate(have_gym = ifelse(grepl("gym", data$amenities), 1, 0)) %>%
  mutate(have_washer = ifelse(grepl("washer", data$amenities), 1, 0)) %>%
  mutate(have_kitchen = ifelse(grepl("kitchen", data$amenities), 1, 0)) %>%
  mutate(have_parking = ifelse(grepl("parking", data$amenities), 1, 0)) %>% 
  mutate(have_elevator = ifelse(grepl("elevator", data$amenities), 1, 0)) %>% 
  mutate(have_laptop_workspac = ifelse(grepl("Laptop friendly workspace", data$amenities), 1, 0)) 
  
#property_type (combine factors)
data <- data.table(data)
data <- data[property_type %in% c("Barn", "Bed and breakfast", "Boat", "Bungalow","Bus","Cabin","Camper/RV","Casa particular (Cuba)", "Castle","Cave","Cottage","Dome house","Earth house","Farm stay","Houseboat","Island","Nature lodge","Other","Tent","Timeshare","Tiny house","Yurt"), property_type:= "Others"]
data <- data[property_type %in% c("Townhouse", "House", "Guesthouse", "Resort","Villa","Tiny house"), property_type:= "House"]
data <- data[property_type %in% c("Apartment","Condominium","Guest suite","Serviced apartment"), property_type:= "Apartment"]
data <- data[property_type %in% c("Hostel","Hotel","Boutique hotel","Aparthotel"), property_type:= "Hotel"]
data <- data[grepl("loft", data$name), property_type := "Loft"]


```

In terms of data cleaning, we have done some simple formatting such as removing dollar sign in the price column. The original data sets include over 30 property types and some of them are quite similar, so we have merged similar ones such as "Apartment" and "Serviced apartment" and reduced to only 5 major types.      

In order to test whether a specific amenity such as shampoo, parking or kitchen will significantly change the price. We have created some new categorical variables that have values of 0s and 1s and named them with "have_shampoo" etc.       


Also, for NLP, we have removed stopwords, punctuations and special characters from the "description" and "summary". We also have extracted their stems for further text analysis.                



```{r}
# average price by room type + boroughs
price_type <- data %>% group_by(neighbourhood_group, room_type) %>% summarise(n=n())
setorderv(price_type, cols = "n", order = -1)
ggplot(price_type, aes(fill=room_type, y=n, x=neighbourhood_group)) +
        geom_bar(position="stack", stat="identity")+
        xlab("Neighbourhood") + ylab("# of Listings")
```


```{r}
#Distribution of Price
ggplot(aes(price), data = data) +labs(  title = "Histogram of price", xlab = "Price") + 
  geom_histogram(alpha=0.8, aes(y=..density..), fill="lightblue", col="white") +
   geom_density( col = "red") + 
  theme_minimal()

#jpeg("rplot.jpg")

ggplot(aes(log(price)), data = data) +labs(  title = "Histogram of log price", xlab = "Price") + 
  geom_histogram(alpha=0.8, aes(y=..density..), fill="lightblue", col="white") +
   geom_density( col = "red") + 
  theme_minimal()

#dev.off()
```



```{r}
# Outliers 
# need to check: accommodates, bathrooms, bedrooms, beds, price, cleaning_fee, minimum_nights, number_of_reviews...
### ATTENTION many 0 bedroom! 
### ATTENTION many outliers in security_deposit
nrow(data)  #48377
# nrow(data %>% filter(beds > 20)) #5
# nrow(data %>% filter(bedrooms > 20)) #1
# nrow(data %>% filter(bedrooms == 1 & bathrooms > 4)) #39    ##1 bedroom & 3+ bathrooms usually comes with price under 100
# nrow(data %>% filter(price > 1500)) #137
# nrow(data %>% filter(cleaning_fee > 200 & minimum_nights < 30)) #381   ##prefer not to use cleaning_fee
# nrow(data %>% filter(minimum_nights > 90)) #361
# hist(data$number_of_reviews)
# hist(data$hosted_years)
data <- data  %>% 
  filter(beds <= 20, bedrooms <= 20,
         bedrooms != 1 | bathrooms <= 4, 
         price < 1500 & price > 0,
         cleaning_fee <= 200 | minimum_nights >= 30,
         minimum_nights <= 90) %>%
  filter(property_type != "Others") #havn't decided
nrow(data)  #37989
```

Looking at the histogram of price, we noticed that there are several properties with abnormally high price which will potentially impact modelling fitting and sample tests. It is natural to remove them from the dataset. Furthermore, we have removed 1 listing that has over 20 bedrooms and 5 listings with more than 20 beds. 


```{r}
#check missing value
colSums(is.na(data))
# data <- data %>%
#   select(-c(weekly_price, square_feet, security_deposit))
data <- data.table(data)
data <- data[is.na(host_is_superhost), host_is_superhost := "0"]
data <- data[!is.na(bathrooms) & !is.na(host_identity_verified) & !is.na(host_listings_count)]
```
delete weekly_price, square_feet

Next, we have to deal with missing values. It is natural to think that a property's area is highly associated with price and hence an efficient predictor. Unfortunately, in our dataset, there are over 90% of the listings that do not have that information. Therefore, we have no choice but delete the "square_feet" variable. But variables like number of bedrooms/bathrooms and room type are still variable, and are able to somewhat represent information of areas. 


# Part 3: Exploratory Date Analysis

```{r}
###neighbourhood_group###
# % of 5 boroughs
dat2 <- data.table(data)
neighb_count <- dat2[, .N, keyby = neighbourhood_group] %>%
                mutate(percent = percent(N / sum(N))) %>% 
                mutate(label = paste(neighbourhood_group, percent))
pie(neighb_count$N, neighb_count$label)
# average price in 5 boroughs
price_avg <- dat2 %>% group_by(neighbourhood_group) %>% 
                 summarise(Avg_price = round(mean(price, na.rm = T),2), N = n())
setorderv(price_avg, "Avg_price",order = -1)
price_avg
#bar plot
barplot(price_avg$Avg_price, names.arg = price_avg$neighbourhood_group, ylim = c(0,200))
text(x = c(0.7,1.9,3.1,4.2,5.5), y = price_avg$Avg_price + 6, labels = paste("$", price_avg$Avg_price))
#hist of price in 5 boroughs
data %>% ggplot(aes(price, color = neighbourhood_group)) + geom_freqpoly()
#boxplot
data %>% ggplot(aes(fct_reorder(neighbourhood_group, -price), price, color = neighbourhood_group)) + geom_boxplot()
#Wilcoxon rank-sum test
kruskal.test(price ~ neighbourhood_group, data)

pairwise.wilcox.test(data$price, data$neighbourhood_group)
```


```{r}
###neighbourhood###
dat_m <- dat2[neighbourhood_group %in% c("Manhattan", "Brooklyn")]
m_count <- dat_m[,.N,keyby = neighbourhood]
setorderv(m_count,"N",order = -1)
m_count
barplot(m_count$N[1:5], names.arg = m_count$neighbourhood[1:5],cex.names = 0.9)
```


```{r}
###room_type###
# average price by room type  
price_type <- dat2 %>% group_by(room_type) %>% summarise(avg_price = round(mean(price, na.rm = T),2))
price_type
barplot(height = price_type$avg_price, names.arg = price_type$room_type)
# average price by room type + boroughs
price_type <- data %>% group_by(neighbourhood_group, room_type) %>% summarise(n=n())
setorderv(price_type, cols = "n", order = -1)
ggplot(price_type, aes(fill=room_type, y=n, x=neighbourhood_group)) +
        geom_bar(position="stack", stat="identity")+
        xlab("Neighbourhood") + ylab("Counts")
#Wilcoxon rank-sum test
kruskal.test(price ~ room_type, data)
#p-value < 2.2e-16, significant differences in price according to room types
```


```{r}
###hosted_years###
#avg hosted years for five boros
#any relationship between hosted years and avg price
data %>% group_by(neighbourhood_group) %>% summarise(avg_hosted_years = mean(hosted_years,na.rm = T),avg_price = mean(price)) %>% arrange(desc(avg_price))
```


Else
```{r}
###cancellation_policy###
#see whether the cancellation policy is strict or moderate
data = data %>%
  mutate(cancelpolicy = strsplit(as.character(data$cancellation_policy),"_"))
how_is_cancellation = function(x){
  ifelse("strict" %in% x,"strict",ifelse("moderate" %in% x,"moderate","flexible"))
}
#for example, see how is the cancellation polict of the first 7 host
#unlist(lapply(data$cancelpolicy,how_is_cancellation)[1:7])
name = data$name[1:7]
cancel = unlist(lapply(data$cancelpolicy,how_is_cancellation)[1:7])
as.data.frame(cbind(name,cancel))
```


```{r}
#neighbourhood
#assume same shape, only differ in location
data %>% ggplot(aes(price, color = neighbourhood_group)) + geom_freqpoly()
kruskal.test(price ~ neighbourhood_group, data)
#with no surprise, the price in at least one borough is different from others
wilcox.test(data[data$neighbourhood_group == 'Staten Island',]$price, data[data$neighbourhood_group == 'Queens',]$price)
#p-value = 0.3391, no significant difference of prices in Staten Island and queens
#two sample test
t.test(data[data$neighbourhood_group == 'Staten Island',]$price,data[data$neighbourhood_group == 'Queens',]$price)
#p-value = 0.9255, no significant difference of prices in Staten Island and queens
#room_type in Manhattan
#assume shape of dist same
data[data$neighbourhood_group == "Manhattan",] %>% ggplot(aes(price, color = room_type)) + geom_freqpoly()
kruskal.test(price~room_type, data = data[data$neighbourhood_group == "Manhattan",])
#p-value < 2.2e-16, significant difference of prices among room type in Manhattan
summary(aov(price~room_type,data[data$neighbourhood_group == "Manhattan",]))
#p-value small, significant difference of prices among room type in Manhattan
Box.test(aov(price~room_type,data[data$neighbourhood_group == "Manhattan",])$residuals)
#error variance is not constant
dwtest(aov(price~room_type,data[data$neighbourhood_group == "Manhattan",]))
#errors are correlated,remedial measures: Transformation, Cochrane-Orcutt Procedure
#property_type in Manhattan
man = data[data$neighbourhood_group == "Manhattan",]
wilcox.test(man[man$property_type == 'Apartment',]$price, 
           man[man$property_type == 'Others',]$price)
#for example, there is no significant differences in prices between apartment and others in Manhattan.
#linear regression
avg_price_hostedyears = data %>% group_by(neighbourhood_group) %>% summarise(avg_hosted_years = mean(hosted_years,na.rm = T),avg_price = mean(price)) %>% arrange(desc(avg_price))
f = lm(avg_price_hostedyears$avg_price~avg_price_hostedyears$avg_hosted_years)
summary(f)
#R square small not so significant relationship btw avg price and avg hosted years
qqnorm(f$residuals)
qqline(f$residuals)
#the residuals are not normally distributed
#predict price
#select all numerial variables
fit <- lm(price ~ host_is_superhost +host_identity_verified + accommodates+ bathrooms + bedrooms + beds +cleaning_fee+minimum_nights+availability_365+ number_of_reviews + instant_bookable + require_guest_phone_verification + hosted_years + number_of_amenities, data)
summary(fit)
sort(car::vif(fit),decreasing = TRUE)
qqnorm(fit$residuals)
#no multicollinearity > 10 variables
#There is skewness, many points off the normal line. Thus the residuals are not normally distributed.
#remedial measures: transformation, robust regression methods
library(lmtest)
dwtest(fit)
#p-value smaller than 0.05, so we reject the null hypothests. The errors are correlated. remedial measures: Transformation, Cochrane-Orcutt Procedure
#model selection
step(fit,direction = "both")
#accommodates,bathrooms, bedrooms,beds,cleaning_fee,min_nights,number of reviews, number of amenities, hosted years and number of amenities
summary(lm(price ~accommodates+ bathrooms + bedrooms + beds +cleaning_fee+minimum_nights+ number_of_reviews + number_of_amenities+hosted_years, man))
# R square 0.4164 bigger than that of the model using all numerical variables.
# Here using less vairables, model better
```


# Part 4: Predictive Modeling     

## Linear Regression

Since the model output, price, is a continuous variable, the first baseline model we have chosen is linear regression 

```{r}
#linear regression
fit1 <- lm(price ~ host_is_superhost + bathrooms + bedrooms + beds + number_of_reviews + instant_bookable + require_guest_phone_verification + hosted_years + number_of_amenities, data)

summary(fit1)
#Adjusted R-squared:  0.2092
#multicollinearity
car::vif(fit1)
data <- data %>% 
  mutate(if_Manhattan = ifelse(data$neighbourhood_group == 'Manhattan', 1, 0)) %>%
  mutate(if_Brooklyn = ifelse(data$neighbourhood_group == 'Brooklyn', 1, 0)) %>%
  mutate(if_Queens = ifelse(data$neighbourhood_group == 'Queens', 1, 0)) %>%
  mutate(if_Bronx = ifelse(data$neighbourhood_group == 'Bronx', 1, 0)) 

fit2 <- lm(price ~ bathrooms + bedrooms + beds + number_of_reviews + 
             hosted_years + number_of_amenities + if_Manhattan + if_Brooklyn + if_Queens + if_Bronx, data)
summary(fit2)
#Adjusted R-squared:  0.3295 
car::vif(fit2)
#robust regression?  ##no conclusion
fit3 <- rlm(price ~ bathrooms + bedrooms + beds + number_of_reviews + 
             hosted_years + number_of_amenities + if_Manhattan + if_Brooklyn + if_Queens + if_Bronx, data)
summary(fit3)
#other method...
```

## Regularized Linear Regression

When number of features is large and suffer from collinearity issues, simple linear model tends to overfit and thus have high variance sample. Hence, we next conducted lasso regression as a means of regularization in order to perform feature selection, reduce variance and decrease our of sample error.

```{r}
### TODO ###
#copy paste lasso code here 
```


## Random Forest

Next we train a random forest regression model since it works well with categorical variables and missing values.
We prepared model data by splitig dataset as 70% train and 30% test and we used OOB (out-of-bag) error as validation error rate.

```{r}
model_data <- data %>%  dplyr::select(host_is_superhost,host_listings_count,
                              host_identity_verified,
                      neighbourhood_group, property_type, room_type, accommodates, 
                      bathrooms, bedrooms, beds, bed_type, price, minimum_nights,
                      availability_365, instant_bookable, cancellation_policy, 
                      require_guest_phone_verification, host_response_time, 
                      number_of_amenities, number_of_verifications, transit_level,
                      rule_Pets, have_pet, have_shampoo, have_gym, have_washer,
                      have_kitchen, have_parking, have_elevator, have_laptop_workspac)

set.seed(0)
split <- rsample::initial_split(model_data, prop = 0.7)
data_train <- training(split)
data_test <- testing(split)
```

Then we fitted a basic random forest model with *ranger* package. We pick number of trees to be 500 as default and  *mtry* parameters, which is number of features to pick for each split, to be $\frac{\# of features}{3}$ as default optimal number.

```{r}
rf <- ranger(
  formula   = price ~ ., 
  data      = data_train, 
  num.trees = 500,
  mtry      = floor((ncol(model_data)-1) / 3))
```


For this random forest model, we achieved `r rf$r.squared` OOB R squared and `r rf$prediction.error` OOB MSE. It already outperformed linear regression, but we still needed to fine tune model parameters to further improve predicting accuracy. Hence we conducted hyperparameter tuning through full grid search.

```{r}
hyper_grid <- expand.grid(
  mtry       = c(6,9,12),
  node_size  = c(3,5,7),
  sampe_size = c(.632, .80),
  OOB_RMSE   = 0
)

for(i in 1:nrow(hyper_grid)) {

  model <- ranger(
    formula         = price ~ .,
    data            = data_train,
    num.trees       = 500,
    mtry            = hyper_grid$mtry[i],
    min.node.size   = hyper_grid$node_size[i],
    sample.fraction = hyper_grid$sampe_size[i],
    seed            = 0
  )

  # add OOB error to grid
  hyper_grid$OOB_RMSE[i] <- sqrt(model$prediction.error)
}
 
hyper_grid %>%
  dplyr::arrange(OOB_RMSE) %>%
  head(10)
```

The grid we constructed contains `r nrow(hyper_grid)` number of parameters combinations.
Our tuning result shows that the optimal parameters to be num.trees = 500, mtry = 9 , 
min.node.size = 3 and sample.fraction = 0.8 for our model.

```{r}
optimal_ranger <- ranger(
  formula         = price ~ ., 
  data            = data_train, 
  num.trees       = 500,
  mtry            = 9,
  min.node.size   = 3,
  sample.fraction = .8,
  importance      = 'impurity'
)
```

```{r}
optimal_ranger$variable.importance %>% 
  tidy() %>% 
  dplyr::arrange(desc(x)) %>%
  dplyr::top_n(15) %>%
  ggplot(aes(reorder(names, x), x)) +
  geom_col(fill = "lightblue") +
  ggtitle("Top 15 important variables")+
  xlab("")+
  ylab("")+
  coord_flip() +
  theme_classic()
```

From the variable importance plot, we can see that top 3 variables picked up by random forest model is 
room_type, accomodates, bathrooms.


```{r}
pred <- predict(optimal_ranger, data_test)
MSE_RF <- mean((pred$predictions - data_test$price)^2)
MSE_RF
```

We achieved final test MSE of `r MSE_RF` by random forest model.



## XGBoost

Then we move on to xgboost model in hope to further improve prediction power. Whereas random forests build an ensemble of deep independent trees, XGBoosts build an ensemble of shallow and weak successive trees with each tree learning and improving on the previous. When combined, these many weak successive trees produce a powerful “committee” that are often hard to beat with other algorithms.

For data preparation, we performed one-hot encoding for categorical variables using *vtreat* package since
XGBoost only works with matrices that contain all numeric variables.

```{r}
# variable names
features <- setdiff(names(data_train), "price")

# Create the treatment plan from the training data
treatplan <- vtreat::designTreatmentsZ(data_train, features, verbose = FALSE)

# Get the "clean" variable names from the scoreFrame
new_vars <- treatplan %>%
  magrittr::use_series(scoreFrame) %>%        
  dplyr::filter(code %in% c("clean", "lev")) %>% 
  magrittr::use_series(varName)     

# Prepare the training data
features_train <- vtreat::prepare(treatplan, data_train, varRestriction = new_vars) %>% as.matrix()
response_train <- data_train$price

# Prepare the test data
features_test <- vtreat::prepare(treatplan, data_test, varRestriction = new_vars) %>% as.matrix()
response_test <- data_test$price
```


```{r}
set.seed(0)

xgb.fit <- xgb.cv(
  data = features_train,
  label = response_train,
  nrounds = 1000,
  nfold = 5,
  objective = "reg:linear",  # for regression models
  verbose = 0,               # silent,
  early_stopping_rounds = 10 # stop if no improvement for 10 consecutive trees
)

xgb.fit$evaluation_log %>%
  dplyr::summarise(
    ntrees.train = which(train_rmse_mean == min(train_rmse_mean))[1],
    rmse.train   = min(train_rmse_mean),
    ntrees.test  = which(test_rmse_mean == min(test_rmse_mean))[1],
    rmse.test   = min(test_rmse_mean)
  )
```

```{r}
# plot error vs number trees
ggplot(xgb.fit$evaluation_log) +
  geom_line(aes(iter, train_rmse_mean), color = "red") +
  geom_line(aes(iter, test_rmse_mean), color = "blue")
```

From the plot above, we can see how training and test RMSE decreases as the number of trees grows. 
And Test RMSE 74.93852 reached minumun as 74.93852. 

```{r, eval=False}
## TODO ## 
## Need to update this grid ##
hyper_grid <- expand.grid(
  eta = c(0.05, 0.1, 0.3),
  max_depth = c(3,5, 7), 
  min_child_weight = c(1, 3, 5), 
  subsample = c(0.65, 0.8), 
  colsample_bytree = c(.8, 1), 
  optimal_trees = 0,               # a place to dump results
  min_RMSE = 0                     # a place to dump results
)

for(i in 1:nrow(hyper_grid)) {
  
  # create parameter list
  params <- list(
    eta = hyper_grid$eta[i],
    max_depth = hyper_grid$max_depth[i],
    min_child_weight = hyper_grid$min_child_weight[i],
    subsample = hyper_grid$subsample[i],
    colsample_bytree = hyper_grid$colsample_bytree[i]
  )
  
  set.seed(0)
  
  # train model
  xgb.tune <- xgb.cv(
    params = params,
    data = features_train,
    label = response_train,
    nrounds = 5000,
    nfold = 5,
    objective = "reg:linear",  # for regression models
    verbose = 0,               # silent,
    early_stopping_rounds = 10 # stop if no improvement for 10 consecutive trees
  )
  
  # add min training error and trees to grid
  hyper_grid2$optimal_trees[i] <- which.min(xgb.tune$evaluation_log$test_rmse_mean)
  hyper_grid2$min_RMSE[i] <- min(xgb.tune$evaluation_log$test_rmse_mean)
} 

hyper_grid %>% 
  dplyr::arrange(min_RMSE) %>%
  head(10)
```

We again tuned our xgoost model parameters through grid search and find the optimal paramters to be :
eta(learning rate) = 0.01, max_depth = 7, min_child_weight, sub_sampling = , colsample_bytree = .

```{r}
## TODO## 
## Need to update this tuned_params
tuned_params <- list(
  eta = 0.01,
  max_depth = 7,
  min_child_weight = 1,
  subsample = 0.65,
  colsample_bytree = 0.8
)

xgb.final <- xgb.cv(
  params = tuned_params,
  data = features_train,
  label = response_train,
  nrounds = 1853,
  nfold = 5,
  objective = "reg:linear",  # for regression models
  verbose = 0,               # silent,
  early_stopping_rounds = 10 # stop if no improvement for 10 consecutive trees
)

pred <- predict(xgb.final, features_test)

# results
MSE_XGBoost <- (caret::RMSE(pred, response_test))^2
MSE_XGBoost
```

We then fed the optimal model parameters to test dataset and achieved test MSE to be AAAAA, which is slightly improved from the random forest model.

```{r}
# create importance matrix
importance_matrix <- xgb.importance(model = xgb.final)

# variable importance plot
xgb.plot.importance(importance_matrix, top_n = 10, measure = "Gain")
```

For XGBoost model, the top 3 influential variables are AAAA, BBBB, CCCC. (Similar or different from random forest???)

```{r}
### TODO ###
## Optional Include LIME visualizaiton 

# # one-hot encode the local observations to be assessed.
# local_obs_onehot <- vtreat::prepare(treatplan, local_obs, varRestriction = new_vars)
# 
# # apply LIME
# explainer <- lime(data.frame(features_train), xgb.fit.final)
# explanation <- explain(local_obs_onehot, explainer, n_features = 5)
# plot_features(explanation)
```



